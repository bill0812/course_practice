set_options(width = img.width, height = img.height)
hist.ggvis <- mtcars %>% ggvis(x = ~mpg) %>% layer_histograms(width=1) %>%
set_options(width = img.width, height = img.height)
hist.ggvis
# Scatter plots
scatter.ggplot <- ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point()
scatter.ggplot
scatter.ggvis <- mtcars %>% ggvis(x = ~wt, y = ~mpg) %>% layer_points() %>%
set_options(width = img.width, height = img.height)
scatter.ggvis
mtcars %>% ggvis(~wt, ~mpg) %>% layer_points() %>%
layer_model_predictions(model = "lm", se = TRUE)
# Scatter plots by group
scatter.ggplot <- ggplot(mtcars, aes(x=wt, y=mpg, colour=cyl)) + geom_point()
scatter.ggplot
# Scatter plots by group
scatter.ggplot <- ggplot(mtcars, aes(x=wt, y=mpg, colour=cyl)) +
geom_point()
scatter.ggplot
scatter.ggvis <- mtcars %>% ggvis(x = ~wt, y = ~mpg, fill = ~cyl) %>%
layer_points() %>% set_options(width = img.width, height = img.height)
scatter.ggvis
mtcars %>%
ggvis(~wt, ~mpg, fill = ~factor(cyl)) %>%
layer_points() %>%
group_by(cyl) %>%
layer_model_predictions(model = "lm", se = TRUE)
# Line plots
line.ggplot <- ggplot(mtcars.mean, aes(x=cyl, y=mpg_mean, colour=am)) + geom_line(aes(group=am))
line.ggplot
# ggvis
# plot1
p <- ggvis(mtcars, x = ~wt, y = ~mpg)
layer_points(p)
# plot2
layer_points(ggvis(mtcars, x = ~wt, y = ~mpg))
# plot3
mtcars %>% ggvis(x = ~wt, y = ~mpg) %>% layer_points()
# why %>%
mtcars %>%
ggvis(x = ~mpg, y = ~disp) %>%
mutate(disp = disp / 61.0237) %>% # convert engine displacment to litres
layer_points()
# Simple, which include primitives like points, lines and rectangles.
mtcars %>% ggvis(~wt, ~mpg) %>% layer_points()
# Compound, which combine data transformations with one or more simple layers.
mtcars %>% ggvis(~mpg) %>% layer_histograms()
# Multiple layers
mtcars %>% ggvis(~wt, ~mpg) %>% layer_smooths() %>% layer_points()
?density
# original density computes kernel density estimates
plot(density(mtcars$wt, kernel="rectangular"))
# original density computes kernel density estimates
plot(density(mtcars$wt, kernel="rectangular"))
plot(density(mtcars$wt, kernel="gaussian"))
plot(density(mtcars$wt, kernel="epanechnikov"))
# interactive density plot
mtcars %>% ggvis(x = ~wt) %>%
layer_densities( adjust = input_slider(.1, 2, value = 1, step = .1, label = "Bandwidth adjustment"),
kernel = input_select( c("Gaussian" = "gaussian", "Epanechnikov" = "epanechnikov", "Rectangular" = "rectangular", "Triangular" = "triangular", "Biweight" = "biweight", "Cosine" = "cosine", "Optcosine" = "optcosine"), label = "Kernel") )
mtcars %>% ggvis(~mpg, ~disp, size = ~vs) %>%
layer_points()
mtcars %>% ggvis(~wt, ~mpg, size := 300, opacity := 0.4) %>%
layer_points()
mtcars %>% ggvis(~wt, ~mpg, size := input_slider(10, 100), opacity := input_slider(0, 1) ) %>%
layer_points()
mtcars %>% ggvis(~mpg, ~disp, size = ~vs) %>%
layer_points()
mtcars %>% ggvis(~wt, ~mpg, size := 300, opacity := 0.4) %>%
layer_points()
mtcars %>% ggvis(~wt, ~mpg, size := input_slider(10, 100), opacity := input_slider(0, 1) ) %>%
layer_points()
slider <- input_slider(10, 1000)
mtcars %>% ggvis(~wt, ~mpg) %>%
layer_points(fill := "red", opacity := 0.5, size := slider)
keys_s <- left_right(10, 1000, step = 50)
mtcars %>% ggvis(~wt, ~mpg, size := keys_s, opacity := 0.5) %>%
layer_points()
keys_s <- left_right(10, 1000, step = 50)
mtcars %>% ggvis(~wt, ~mpg, size := keys_s, opacity := 0.5) %>%
layer_points()
install.packages("shiny")
install.packages("shiny")
install.packages("shiny")
install.packages("shiny")
install.packages("shiny")
library(shiny)
runExample("01_hello")
runApp('Dropbox/13_NCCU/courses/DataScienceInPractice_資料科學實務/1071/codes/code07/runApp1.R')
runApp('Dropbox/13_NCCU/courses/DataScienceInPractice_資料科學實務/1071/codes/code07/runApp1.R')
runApp('Dropbox/13_NCCU/courses/DataScienceInPractice_資料科學實務/1071/codes/code07/runApp1.R')
install.packages("tidyverse")
library(tidyverse)
# plot
ggplot(data = diamonds) +
geom_bar(mapping = aes(x = cut))
head(diamonds)
# count
diamonds %>%
count(cut)
summary(di
)
summary(diamonds)
# count
diamonds %>%
count(cut)
# A variable is continuous
ggplot(data = diamonds) +
geom_histogram(mapping = aes(x = carat), binwidth = 0.5)
# count
diamonds %>%
count(cut_width(carat, 0.5))
#overlay multiple histograms in the same plot
smaller <- diamonds %>%
filter(carat < 3)
ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) +
geom_freqpoly(binwidth = 0.1)
# oevrall
ggplot(diamonds) +
geom_histogram(mapping = aes(x = y), binwidth = 0.5)
# zoom in
ggplot(diamonds) +
geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
coord_cartesian(ylim = c(0, 50))
# oevrall
ggplot(diamonds) +
geom_histogram(mapping = aes(x = y), binwidth = 0.5)
# zoom in
ggplot(diamonds) +
geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
coord_cartesian(ylim = c(0, 50))
unusual <- diamonds %>%
filter(y < 3 | y > 20) %>%
arrange(y)
unusual
diamonds2 <- diamonds %>% filter(between(y, 3, 20))
diamonds2
diamonds2 <- diamonds %>% mutate(y = ifelse(y < 3 | y > 20, NA, y))
ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + geom_point()
ggplot(data = diamonds, mapping = aes(x = x, y = y)) + geom_point()
# how the price of a diamond varies with its quality?
# It???s hard to see the difference in distribution because the overall counts differ so much:
ggplot(data = diamonds, mapping = aes(x = price)) +
geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
# use density, that the area under each frequency polygon is one.
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) +
geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
# by boxplot
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
geom_boxplot()
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) +  geom_boxplot()
ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))
ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) + coord_flip()
ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy), notch = T) + coord_flip()
?boxplot
ggplot(data = diamonds) + geom_count(mapping = aes(x = cut, y = color))
diamonds %>% count(color, cut) %>% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = n))
ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price))
ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)
ggplot(data = smaller) + geom_bin2d(mapping = aes(x = carat, y = price))
ggplot(data = smaller, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))
# won???t supply those names. That saves typing, and, by reducing the amount of boilerplate, makes it easier to see what???s different between plots
ggplot(data = faithful, mapping = aes(x = eruptions)) +
geom_freqpoly(binwidth = 0.25)
ggplot(faithful, aes(eruptions)) +
geom_freqpoly(binwidth = 0.25)
setwd("~/Dropbox/13_NCCU/courses/DataScienceInPractice_資料科學實務/1071/codes/code11/")
setwd("~/Dropbox/13_NCCU/courses/DataScienceInPractice_資料科學實務/1071/codes/code10/")
setwd("./books/")
library(arules)
install.packages("arules")
library(arules)
#Reading in the book data
bookbaskets <- read.transactions("bookdata.tsv.gz", format="single",
sep="\t", cols=c("userid", "title"), rm.duplicates=T)
class(bookbaskets)
bookbaskets
dim(bookbaskets)
colnames(bookbaskets)[1:5]
rownames(bookbaskets)[1:5]
# the distribution of transaction sizes
basketSizes <- size(bookbaskets)
summary(basketSizes)
# Examining data
quantile(basketSizes, probs=seq(0,1,0.1))
library(ggplot2)
ggplot(data.frame(count=basketSizes)) +
geom_density(aes(x=count), binwidth=1) +
scale_x_log10()
# Frequent books
bookFreq <- itemFrequency(bookbaskets)
sum(bookFreq)
bookCount <- (bookFreq/sum(bookFreq))*sum(basketSizes)
summary(bookCount)
orderedBooks <- sort(bookCount, decreasing=T)
orderedBooks[1:10]
orderedBooks[1]/dim(bookbaskets)[1]
# Finding the association rules
#   Preprocessing
bookbaskets_use <- bookbaskets[basketSizes > 1]
dim(bookbaskets_use)
dim(bookbaskets)
# Decide support & confidence
100/dim(bookbaskets_use)[1]
rules <- apriori(bookbaskets_use, parameter =list(support = 0.002, confidence=0.75))
summary(rules)
# Scoring rules
measures <- interestMeasure(rules,
measure=c("coverage", "fishersExactTest"),
transactions=bookbaskets_use)
# The five most confident rules discovered in the data
inspect(head((sort(rules, by="confidence")), n=5))
# Finding rules with restrictions
brules <- apriori(bookbaskets_use,
parameter =list(support = 0.001, confidence=0.6),
appearance=list(rhs=c("The Lovely Bones: A Novel"),
default="lhs"))
summary(brules)
# Inspecting rules
brulesConf <- sort(brules, by="confidence")
inspect(head(brulesConf, n=5))
inspect(head(lhs(brulesConf), n=5))
# Inspecting rules with restrictions
brulesSub <- subset(brules, subset=!(lhs %in% "Lucky : A Memoir"))
brulesConf <- sort(brulesSub, by="confidence")
inspect(head(lhs(brulesConf), n=5))
setwd("../../code11/KDD2009/")
d <- read.table('orange_small_train.data.gz',  	# Note: 1
header=T,
sep='\t',
na.strings=c('NA','')) 	# Note: 2
churn <- read.table('orange_small_train_churn.labels.txt',
header=F,sep='\t') 	# Note: 3
d$churn <- churn$V1 	# Note: 4
appetency <- read.table('orange_small_train_appetency.labels.txt',
header=F,sep='\t')
d$appetency <- appetency$V1 	# Note: 5
upselling <- read.table('orange_small_train_upselling.labels.txt',
header=F,sep='\t')
d$upselling <- upselling$V1 	# Note: 6
set.seed(729375) 	# Note: 7
d$rgroup <- runif(dim(d)[[1]])
dTrainAll <- subset(d,rgroup<=0.9)
dTest <- subset(d,rgroup>0.9) 	# Note: 8
outcomes=c('churn','appetency','upselling')
vars <- setdiff(colnames(dTrainAll),
c(outcomes,'rgroup'))
head(vars)
catVars <- vars[sapply(dTrainAll[,vars],class) %in%
c('factor','character')] 	# Note: 9
numericVars <- vars[sapply(dTrainAll[,vars],class) %in%
c('numeric','integer')] 	# Note: 10
rm(list=c('d','churn','appetency','upselling')) 	# Note: 11
head(catVars)
head(numericVarsVars)
head(numericVars)
outcome <- 'churn' 	# Note: 12
pos <- '1' 	# Note: 13
useForCal <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0 	# Note: 14
dCal <- subset(dTrainAll,useForCal)
dTrain <- subset(dTrainAll,!useForCal)
table218 <- table(
Var218=dTrain[,'Var218'], 	# Note: 1
churn=dTrain[,outcome], 	# Note: 2
useNA='ifany') 	# Note: 3
print(table218)
print(table218[,2]/(table218[,1]+table218[,2]))
mkPredC <- function(outCol,varCol,appCol) { 	# Note: 1
pPos <- sum(outCol==pos)/length(outCol) 	# Note: 2
naTab <- table(as.factor(outCol[is.na(varCol)]))
pPosWna <- (naTab/sum(naTab))[pos] 	# Note: 3
vTab <- table(as.factor(outCol),varCol)
pPosWv <- (vTab[pos,]+1.0e-3*pPos)/(colSums(vTab)+1.0e-3) 	# Note: 4
pred <- pPosWv[appCol] 	# Note: 5
pred[is.na(appCol)] <- pPosWna 	# Note: 6
pred[is.na(pred)] <- pPos 	# Note: 7
pred 	# Note: 8
}
for(v in catVars) {
pi <- paste('pred',v,sep='')
dTrain[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dTrain[,v])
dCal[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dCal[,v])
dTest[,pi] <- mkPredC(dTrain[,outcome],dTrain[,v],dTest[,v])
}
####################################################################
library('ROCR')
calcAUC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'auc')
as.numeric(perf@y.values)
}
for(v in catVars) {
pi <- paste('pred',v,sep='')
aucTrain <- calcAUC(dTrain[,pi],dTrain[,outcome])
if(aucTrain>=0.8) {
aucCal <- calcAUC(dCal[,pi],dCal[,outcome])
print(sprintf("%s, trainAUC: %4.3f calibrationAUC: %4.3f",
pi,aucTrain,aucCal))
}
}
v<-"Var7"
outCol<-dTrain[,outcome]
varCol<-dTrain[,v]
appCol<-dTrain[,v]
cuts <- unique(as.numeric(quantile(varCol,
probs=seq(0, 1, 0.1),na.rm=T)))
varC <- cut(varCol,cuts)
appC <- cut(appCol,cuts)
head(varCol)
head(cuts)
varC <- cut(varCol,cuts)
head(varC)
mkPredN <- function(outCol,varCol,appCol) {
cuts <- unique(as.numeric(quantile(varCol,
probs=seq(0, 1, 0.1),na.rm=T)))
varC <- cut(varCol,cuts)
appC <- cut(appCol,cuts)
mkPredC(outCol,varC,appC)
}
for(v in numericVars) {
pi <- paste('pred',v,sep='')
dTrain[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dTrain[,v])
dTest[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dTest[,v])
dCal[,pi] <- mkPredN(dTrain[,outcome],dTrain[,v],dCal[,v])
aucTrain <- calcAUC(dTrain[,pi],dTrain[,outcome])
if(aucTrain>=0.55) {
aucCal <- calcAUC(dCal[,pi],dCal[,outcome])
print(sprintf("%s, trainAUC: %4.3f calibrationAUC: %4.3f",
pi,aucTrain,aucCal))
}
}
library('ggplot2')
ggplot(data=dCal) +
geom_density(aes(x=predVar126,color=as.factor(churn)))
ggplot(data=dCal) +
geom_density(aes(x=predVar81,color=as.factor(churn)))
var <- 'Var217'
aucs <- rep(0,100)
for(rep in 1:length(aucs)) {   	# Note: 1
useForCalRep <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0  	# Note: 2
predRep <- mkPredC(dTrainAll[!useForCalRep,outcome],  	# Note: 3
dTrainAll[!useForCalRep,var],
dTrainAll[useForCalRep,var])
aucs[rep] <- calcAUC(predRep,dTrainAll[useForCalRep,outcome])  	# Note: 4
}
mean(aucs)
## [1] 0.5556656
sd(aucs)
fCross <- function() {
useForCalRep <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0
predRep <- mkPredC(dTrainAll[!useForCalRep,outcome],
dTrainAll[!useForCalRep,var],
dTrainAll[useForCalRep,var])
calcAUC(predRep,dTrainAll[useForCalRep,outcome])
}
aucs <- replicate(100,fCross())
mean(aucs)
## [1] 0.5556656
sd(aucs)
ptm <- proc.time()
aucs <- replicate(100,fCross())
proc.time() - ptm
ptm <- proc.time()
aucs <- rep(0,100)
for(rep in 1:length(aucs)) {
useForCalRep <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0
predRep <- mkPredC(dTrainAll[!useForCalRep,outcome],
dTrainAll[!useForCalRep,var],
dTrainAll[useForCalRep,var])
aucs[rep] <- calcAUC(predRep,dTrainAll[useForCalRep,outcome])
}
proc.time() - ptm
ptm <- proc.time()
aucs <- replicate(1000,fCross())
proc.time() - ptm
ptm <- proc.time()
aucs <- rep(0,1000)
for(rep in 1:length(aucs)) {
useForCalRep <- rbinom(n=dim(dTrainAll)[[1]],size=1,prob=0.1)>0
predRep <- mkPredC(dTrainAll[!useForCalRep,outcome],
dTrainAll[!useForCalRep,var],
dTrainAll[useForCalRep,var])
aucs[rep] <- calcAUC(predRep,dTrainAll[useForCalRep,outcome])
}
proc.time() - ptm
pPos <- sum(dTrain[,outcome]==pos)/length(dTrain[,outcome])
nBayes <- function(pPos,pf) { 	# Note: 1
pNeg <- 1 - pPos
smoothingEpsilon <- 1.0e-5
scorePos <- log(pPos + smoothingEpsilon) +
rowSums(log(pf/pPos + smoothingEpsilon)) 	# Note: 2
scoreNeg <- log(pNeg + smoothingEpsilon) +
rowSums(log((1-pf)/(1-pPos) + smoothingEpsilon)) 	# Note: 3
m <- pmax(scorePos,scoreNeg)
expScorePos <- exp(scorePos-m)
expScoreNeg <- exp(scoreNeg-m) 	# Note: 4
expScorePos/(expScorePos+expScoreNeg) 	# Note: 5
}
scoreNeg <- log(pNeg + smoothingEpsilon) +
rowSums(log((1-pf)/(1-pPos) + smoothingEpsilon)) 	# Note: 3
m <- pmax(scorePos,scoreNeg)
pPos <- sum(dTrain[,outcome]==pos)/length(dTrain[,outcome])
nBayes <- function(pPos,pf) { 	# Note: 1
pNeg <- 1 - pPos
smoothingEpsilon <- 1.0e-5
scorePos <- log(pPos + smoothingEpsilon) +
rowSums(log(pf/pPos + smoothingEpsilon)) 	# Note: 2
scoreNeg <- log(pNeg + smoothingEpsilon) +
rowSums(log((1-pf)/(1-pPos) + smoothingEpsilon)) 	# Note: 3
m <- pmax(scorePos,scoreNeg)
expScorePos <- exp(scorePos-m)
expScoreNeg <- exp(scoreNeg-m) 	# Note: 4
expScorePos/(expScorePos+expScoreNeg) 	# Note: 5
}
pVars <- paste('pred',c(numericVars,catVars),sep='')
dTrain$nbpredl <- nBayes(pPos,dTrain[,pVars])
dCal$nbpredl <- nBayes(pPos,dCal[,pVars])
dTest$nbpredl <- nBayes(pPos,dTest[,pVars]) 	# Note: 6
print(calcAUC(dTrain$nbpredl,dTrain[,outcome]))
## [1] 0.9757348
print(calcAUC(dCal$nbpredl,dCal[,outcome]))
## [1] 0.5995206
print(calcAUC(dTest$nbpredl,dTest[,outcome]))
library('e1071')
install.packages("e1071")
lVars <- c(catVars,numericVars)
ff <- paste('as.factor(',outcome,'>0) ~ ',
paste(lVars,collapse=' + '),sep='')
ff
nbmodel <- naiveBayes(as.formula(ff),data=dTrain)
dTrain$nbpred <- predict(nbmodel,newdata=dTrain,type='raw')[,'TRUE']
dCal$nbpred <- predict(nbmodel,newdata=dCal,type='raw')[,'TRUE']
dTest$nbpred <- predict(nbmodel,newdata=dTest,type='raw')[,'TRUE']
calcAUC(dTrain$nbpred,dTrain[,outcome])
## [1] 0.4643591
calcAUC(dCal$nbpred,dCal[,outcome])
## [1] 0.5544484
calcAUC(dTest$nbpred,dTest[,outcome])
logLikelyhood <- function(outCol,predCol) { 	# Note: 1
sum(ifelse(outCol==pos,log(predCol),log(1-predCol)))
}
selVars <- c()
minStep <- 5
baseRateCheck <- logLikelyhood(dCal[,outcome],
sum(dCal[,outcome]==pos)/length(dCal[,outcome]))
for(v in catVars) {  	# Note: 2
pi <- paste('pred',v,sep='')
liCheck <- 2*((logLikelyhood(dCal[,outcome],dCal[,pi]) -
baseRateCheck))
if(liCheck>minStep) {
print(sprintf("%s, calibrationScore: %g",
pi,liCheck))
selVars <- c(selVars,pi)
}
}
selVars
for(v in numericVars) { 	# Note: 3
pi <- paste('pred',v,sep='')
liCheck <- 2*((logLikelyhood(dCal[,outcome],dCal[,pi]) -
baseRateCheck))
if(liCheck>=minStep) {
print(sprintf("%s, calibrationScore: %g",
pi,liCheck))
selVars <- c(selVars,pi)
}
}
library('class')
knnTrain <- dTrain[,selVars]  	# Note: 1
knnCl <- dTrain[,outcome]==pos 	# Note: 2
knnPred <- function(df) { 	# Note: 3
knnDecision <- knn(knnTrain,df,knnCl,k=nK,prob=T)
ifelse(knnDecision==TRUE, 	# Note: 4
attributes(knnDecision)$prob,
1-(attributes(knnDecision)$prob))
}
print(calcAUC(knnPred(dTrain[,selVars]),dTrain[,outcome]))
## [1] 0.7443927
print(calcAUC(knnPred(dCal[,selVars]),dCal[,outcome]))
knnPred
print(calcAUC(knnPred(dTrain[,selVars]),dTrain[,outcome]))
## [1] 0.7443927
print(calcAUC(knnPred(dCal[,selVars]),dCal[,outcome]))
## [1] 0.7119394
print(calcAUC(knnPred(dTest[,selVars]),dTest[,outcome]))
dCal$kpred <- knnPred(dCal[,selVars])
nK <- 200
knnTrain <- dTrain[,selVars]  	# Note: 1
knnCl <- dTrain[,outcome]==pos 	# Note: 2
knnPred <- function(df) { 	# Note: 3
knnDecision <- knn(knnTrain,df,knnCl,k=nK,prob=T)
ifelse(knnDecision==TRUE, 	# Note: 4
attributes(knnDecision)$prob,
1-(attributes(knnDecision)$prob))
}
print(calcAUC(knnPred(dTrain[,selVars]),dTrain[,outcome]))
## [1] 0.7443927
print(calcAUC(knnPred(dCal[,selVars]),dCal[,outcome]))
dCal$kpred <- knnPred(dCal[,selVars])
ggplot(data=dCal) +
geom_density(aes(x=kpred,
color=as.factor(churn),linetype=as.factor(churn)))
plotROC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'tpr','fpr')
pf <- data.frame(
FalsePositiveRate=perf@x.values[[1]],
TruePositiveRate=perf@y.values[[1]])
ggplot() +
geom_line(data=pf,aes(x=FalsePositiveRate,y=TruePositiveRate)) +
geom_line(aes(x=c(0,1),y=c(0,1)))
}
print(plotROC(knnPred(dTest[,selVars]),dTest[,outcome]))
catVars
head(dCal)
